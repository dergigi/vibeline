# Ollama model configuration
# You can use different models for different tasks
# For better performance, use tinyllama (smaller, faster)
# For better quality, use llama3 or any ollama model: https://ollama.com/search
OLLAMA_EXTRACT_MODEL=tinyllama
OLLAMA_SUMMARIZE_MODEL=tinyllama
OLLAMA_DEFAULT_MODEL=tinyllama

# Ollama connection configuration (optional)
# OLLAMA_HOST=http://localhost:11434

# Path configuration
VOICE_MEMOS_DIR=VoiceMemos

# Whisper model configuration
WHISPER_MODEL=base.en
